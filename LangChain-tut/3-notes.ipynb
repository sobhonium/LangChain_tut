{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d9886-dedb-4288-a94c-502569f77cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d58d7f-a017-4e6b-89cd-b78cd06706ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1st method: using .env file.\n",
    "load_dotenv()\n",
    "# Access them using os.getenv or os.environ\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 2nd method: using hard code\n",
    "# api_key = \"<put the api key here>\"\n",
    "# if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#     os.environ[\"GROQ_API_KEY\"] = api_key #getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51245ab0-464e-425a-b479-7bd9de1d4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8817e-a063-4914-9f07-2d0c4cee5c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ac67e43-ee47-4798-99bf-964738cdf360",
   "metadata": {},
   "source": [
    "## LLMChain vs SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2869a108-f075-4b32-a73b-0a0b8f39ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be filled ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ea39d-27ea-4825-9b1f-129a5731ed50",
   "metadata": {
    "tags": []
   },
   "source": [
    "##   llm.run() vs and llm.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9fc77d-3b28-4965-9870-50024589f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be filled..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52148e58-3491-4298-ab99-177ffc24b83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181faf0d-973b-4fa3-86ea-aecf30417755",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ChatPromptTemplate vs PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4669c-7997-47c8-b6fb-18744cf41fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c33b0741-efe2-43da-9cf0-1a4f38f2ff41",
   "metadata": {
    "tags": []
   },
   "source": [
    "* What is **PromptTemplate**?\n",
    "\n",
    "    A template for traditional completion models, like text-davinci-003 or OpenAI() in text mode.\n",
    "    - used when:\n",
    "    You’re using text completion models\n",
    "    You want a simple string prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* What is **ChatPromptTemplate**?\n",
    "\n",
    "    A template for chat models, like gpt-3.5-turbo or gpt-4, which expect structured message history (system, user, assistant messages).\n",
    "\n",
    "\n",
    "\n",
    "    - used when:\n",
    "    You’re using chat-based models\n",
    "    You want to structure a conversation with multiple roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f5895-9547-4600-a0ea-2368bfaded50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532bd9f4-1269-4a20-af5a-3a7e54ee6f56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Japan?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "prompt.format(country=\"Japan\")\n",
    "# Output: \"What is the capital of Japan?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16198d-dc7a-40dd-88ea-96e6530e3e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94715ed-988b-4fc7-845a-ddc0ed34cfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379c3060-f922-44e8-bca4-ce97d828f7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the capital of Japan?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"What is the capital of {country}?\")\n",
    "])\n",
    "prompt.format_messages(country=\"Japan\")\n",
    "# Output: list of ChatMessages (SystemMessage, HumanMessage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9711ca6-cedf-47b2-8a8f-fc2ea246284b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c64d2f95-9aa3-4ddb-adb6-7e684ce59748",
   "metadata": {},
   "source": [
    "despite ChatPromptTemplate being use as explicity like bellow:\n",
    "\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a critical reasoning assistant.\"),\n",
    "    (\"user\", \"Given the following argument... Argument: {argument}\")\n",
    "])\n",
    "\n",
    "you can implicity put roles in tamples:\n",
    "    \n",
    "    \n",
    "fallacy_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a critical reasoning assistant.\n",
    "\n",
    "Given the following argument, do the following:\n",
    "- Detect if there is a logical fallacy.\n",
    "- Identify the fallacy type (if any).\n",
    "- Rewrite the argument to eliminate the fallacy but keep the main point.\n",
    "\n",
    "Respond in JSON format with keys: fallacy_detected (true/false), fallacy_type, revised_text.\n",
    "\n",
    "Argument: \"{argument}\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91caea4b-88f7-42db-8429-9e122df3f61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1993c7b8-6d39-472f-a93f-372588dc907e",
   "metadata": {},
   "source": [
    "| Feature                         | `PromptTemplate`                        | `ChatPromptTemplate`                             |\n",
    "|----------------------------------|-----------------------------------------|--------------------------------------------------|\n",
    "| Model Type                      | Text completion (e.g., `text-davinci-003`) | Chat models (e.g., `gpt-3.5-turbo`)            |\n",
    "| Output                          | Single formatted string                 | List of structured `ChatMessage` objects         |\n",
    "| Multi-turn interactions         | ❌ No (flat prompt)                     | ✅ Yes (role-based message turns)                |\n",
    "| Message roles (`system`, `user`, `assistant`) | ❌ No                          | ✅ Yes                                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e311a-4d6b-48d1-ae88-d0fc52487ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f853f4-18c0-4e89-9e1b-d4f8bd672069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# so what are completion or chat models and their differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eabdaa0-b268-474a-8d36-ec1e3d55dd2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "| Feature                          | Completion Models                          | Chat Models                                     |\n",
    "|----------------------------------|---------------------------------------------|--------------------------------------------------|\n",
    "| Input Format                     | Plain text string                          | List of messages with roles (system/user/etc.)  |\n",
    "| Designed for                     | One-shot or few-shot tasks                 | Conversational interactions                     |\n",
    "| Model Examples                   | `text-davinci-003`, `GPT-Neo`, `GPT-J`     | `gpt-3.5-turbo`, `gpt-4`, `Claude`, `Gemini`    |\n",
    "| Supports roles (user/system)     | ❌ No                                       | ✅ Yes                                           |\n",
    "| Output Style                     | Text continuation                          | Role-based response from assistant              |\n",
    "| Memory-aware                     | ❌ Not naturally                            | ✅ Yes (chat history maintained)                 |\n",
    "| Performance (for chat tasks)     | ⚠️ Often worse                             | ✅ Optimized for dialogue                        |\n",
    "| Needs manual formatting          | ✅ Yes                                      | ❌ No                                            |\n",
    "| Hard to manage multi-turn input  | ✅ Yes                                      | ❌ No                                            |\n",
    "| Best for                         | Simple or isolated tasks                   | Interactive, multi-turn conversations           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e079b9-abff-43ac-b791-5173a1969e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee907b-600d-493e-80e4-d1fad51ace66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "67a2096f-d968-473e-88eb-159fd571377d",
   "metadata": {},
   "source": [
    "When working with chat models, there are terms we see or hear like: MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7018f023-d4d7-44f6-83b9-5e5608d3c6ae",
   "metadata": {},
   "source": [
    "So, what is MessagesPlaceholder?\n",
    "\n",
    "MessagesPlaceholder is a powerful tool in LangChain, especially when working with chat models and memory.\n",
    "\n",
    "MessagesPlaceholder is used inside a ChatPromptTemplate (not PromptTemplate as mentioned above) to represent a dynamic list of chat messages, such as:\n",
    "\n",
    "- Conversation history (from memory)\n",
    "- Dialogue stored elsewhere\n",
    "- Messages generated or passed at runtime\n",
    "\n",
    "It acts as a slot where a list of BaseMessage objects (e.g., HumanMessage, AIMessage, SystemMessage) will be inserted during execution."
   ]
  },
  {
   "cell_type": "raw",
   "id": "097c1f23-1ea9-48fd-9d99-80a4994127bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29175274-24db-4c3d-93a8-8ad20fb9d218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1k/dtgfp49j1f92ysb1xwlxsnsm0000gp/T/ipykernel_42051/1803211992.py:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n",
      "/var/folders/1k/dtgfp49j1f92ysb1xwlxsnsm0000gp/T/ipykernel_42051/1803211992.py:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n",
      "/var/folders/1k/dtgfp49j1f92ysb1xwlxsnsm0000gp/T/ipykernel_42051/1803211992.py:20: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(input=\"What's the capital of France?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help! According to our chat history, this is the first message you've sent. Therefore, I'll provide the answer: The capital of France is Paris.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "response = chain.run(input=\"What's the capital of France?\")\n",
    "# Automatically stores and injects conversation history\n",
    "\n",
    "\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e13c028-7b76-4345-aca8-c69dc231e856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to our chat history, you've asked about the capital of France, which is Paris. As for the population, Paris has a population of approximately 2.1 million people within its city limits. However, the larger metropolitan area of Paris has a population of over 12.2 million people.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.run(input=\"What's the population of that?\")\n",
    "\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9cb769-a5ac-4612-81e1-43c5c66cc2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1650ee9-38ae-4ee4-b0de-c416856acb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now the question is how to see the moemory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaf6b50d-e16c-4db7-abc4-b55628cf7626",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What's the capital of France?\n",
      "Ai: I'd be happy to help! According to our chat history, this is the first message you've sent. Therefore, I'll provide the answer: The capital of France is Paris.\n",
      "Human: What's the population of that?\n",
      "Ai: According to our chat history, you've asked about the capital of France, which is Paris. As for the population, Paris has a population of approximately 2.1 million people within its city limits. However, the larger metropolitan area of Paris has a population of over 12.2 million people.\n"
     ]
    }
   ],
   "source": [
    "for msg in memory.chat_memory.messages:\n",
    "    print(f\"{msg.type.capitalize()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d81a79e-35fb-46a1-8f34-f13fd1f82e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can see that both turns in messages are stored in mem now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a089c5-7bd1-4291-8d16-10a004dfd65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c4dfcf-34bd-41ae-8df7-4eab16ab632b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chain.run(input=\"the most famouse person ever lived there?\")\n",
    "response = chain.run(input=\"is is larger than London (by land)?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb25d1a-3526-49b9-8989-8b1cd196af1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4d7db4-fd76-4e98-b74d-78fb2c035f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What's the capital of France?\n",
      "Ai: I'd be happy to help! According to our chat history, this is the first message you've sent. Therefore, I'll provide the answer: The capital of France is Paris.\n",
      "Human: What's the population of that?\n",
      "Ai: According to our chat history, you've asked about the capital of France, which is Paris. As for the population, Paris has a population of approximately 2.1 million people within its city limits. However, the larger metropolitan area of Paris has a population of over 12.2 million people.\n",
      "Human: the most famouse person ever lived there?\n",
      "Ai: According to our chat history, you've asked about the capital of France, which is Paris, and also about the population of Paris. As for your latest question, one of the most famous people to have lived in Paris is Vincent van Gogh, a Dutch post-impressionist artist who spent time in the city and created many of his famous works there.\n",
      "Human: is is larger than London (by land)?\n",
      "Ai: According to our chat history, you've asked about the capital of France, population of Paris, and most famous person to have lived in Paris. As for your latest question, Paris is smaller than London in terms of land area. Paris has a total area of 105.4 km² (40.7 sq mi), whereas London has a total area of 1,579 km² (610 sq mi).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for msg in memory.chat_memory.messages:\n",
    "    print(f\"{msg.type.capitalize()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddeb79-40f7-4879-b6bc-2cb4f8e95c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3120087f-fec0-4897-ad7f-092de38a590d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# how many messages so far are there in mem?\n",
    "print(len(memory.chat_memory.messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17bf487-34de-48fc-a382-e45b13705eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eabce1cb-25e0-4d32-bbd6-25e642640ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To clear memory: (uncomment if you want this)\n",
    "# memory.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15eb24d-98b0-4164-a737-0d4a8b170e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41b6f053-0b2a-4a68-ba91-d132f357e98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save messages manually:\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "memory.chat_memory.add_user_message(\"What is the best place there to visit?\")\n",
    "memory.chat_memory.add_ai_message(\"House of Meoosiso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e9cbe-09d6-4253-b1b1-b6cb57aeaec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09cfe61-2e9d-423f-9cc3-19e0890b3ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What's the capital of France?\n",
      "Ai: I'd be happy to help! According to our chat history, this is the first message you've sent. Therefore, I'll provide the answer: The capital of France is Paris.\n",
      "Human: What's the population of that?\n",
      "Ai: According to our chat history, you've asked about the capital of France, which is Paris. As for the population, Paris has a population of approximately 2.1 million people within its city limits. However, the larger metropolitan area of Paris has a population of over 12.2 million people.\n",
      "Human: the most famouse person ever lived there?\n",
      "Ai: According to our chat history, you've asked about the capital of France, which is Paris, and also about the population of Paris. As for your latest question, one of the most famous people to have lived in Paris is Vincent van Gogh, a Dutch post-impressionist artist who spent time in the city and created many of his famous works there.\n",
      "Human: is is larger than London (by land)?\n",
      "Ai: According to our chat history, you've asked about the capital of France, population of Paris, and most famous person to have lived in Paris. As for your latest question, Paris is smaller than London in terms of land area. Paris has a total area of 105.4 km² (40.7 sq mi), whereas London has a total area of 1,579 km² (610 sq mi).\n",
      "Human: What is the best place there to visit?\n",
      "Ai: House of Meoosiso\n"
     ]
    }
   ],
   "source": [
    "for msg in memory.chat_memory.messages:\n",
    "    print(f\"{msg.type.capitalize()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f295c0c2-d2ca-466d-acbe-191c48c37c59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize for the mistake earlier! As for your question, the best place to visit in Paris is a matter of personal preference, but some of the most popular and iconic attractions include the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Arc de Triomphe, and the Champs-Élysées.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "response = chain.run(input=\"What is the best place there to visit?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4034b-79d0-47c9-8b75-c95273b8c389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1efd15-260f-4c94-8910-7f3268db1737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait! it does not remember to use hitsoty it just keep them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39d49b-da8d-43d0-88dd-9d162ad46853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "015abe10-f80c-49f8-9229-d2feb144b1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a helpful assistant. first look at the history of chat messages and then answer based on that.\n",
      "human: What's the capital of France?\n",
      "ai: I'd be happy to help! According to our chat history, this is the first message you've sent. Therefore, I'll provide the answer: The capital of France is Paris.\n",
      "human: What's the population of that?\n",
      "ai: According to our chat history, you've asked about the capital of France, which is Paris. As for the population, Paris has a population of approximately 2.1 million people within its city limits. However, the larger metropolitan area of Paris has a population of over 12.2 million people.\n",
      "human: the most famouse person ever lived there?\n",
      "ai: According to our chat history, you've asked about the capital of France, which is Paris, and also about the population of Paris. As for your latest question, one of the most famous people to have lived in Paris is Vincent van Gogh, a Dutch post-impressionist artist who spent time in the city and created many of his famous works there.\n",
      "human: is is larger than London (by land)?\n",
      "ai: According to our chat history, you've asked about the capital of France, population of Paris, and most famous person to have lived in Paris. As for your latest question, Paris is smaller than London in terms of land area. Paris has a total area of 105.4 km² (40.7 sq mi), whereas London has a total area of 1,579 km² (610 sq mi).\n",
      "human: What is the best place there to visit?\n",
      "ai: House of Meoosiso\n",
      "human: What is the best place there to visit?\n",
      "ai: I apologize for the mistake earlier! As for your question, the best place to visit in Paris is a matter of personal preference, but some of the most popular and iconic attractions include the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Arc de Triomphe, and the Champs-Élysées.\n",
      "human: What is the best place there to visit?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Prepare the input\n",
    "inputs = {\"input\": \"What is the best place there to visit?\", \"history\": memory.chat_memory.messages}\n",
    "\n",
    "# Format prompt manually\n",
    "messages = prompt.format_messages(**inputs)\n",
    "\n",
    "for m in messages:\n",
    "    print(f\"{m.type}: {m.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effa8a4-4f1a-4385-8ff6-70fb768385b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb734444-752a-4be2-9ab3-c295640df831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "131f870b-634d-4e60-9a83-e91257afd9ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "As shown above, it keeps the hisotry but does not use them in future chat messages to use them. so what can we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee5a10-3078-47d3-a344-8e130aaf3e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c01f5079-7e5e-4f06-8dd7-c4b3f314b020",
   "metadata": {},
   "source": [
    "1) option1: Use ConversationSummaryMemory for Fact Recall\n",
    "         ConversationSummaryMemory: (for summarizing and remembering key points in a chat)\n",
    "\n",
    "2) option2: Use RetrievalQA for Fact Recall (later in RAGs we will explain it)\n",
    "\n",
    "Best when:\n",
    "\n",
    "   - You want the model to answer based on stored knowledge (e.g., facts, FAQs, history).\n",
    "   - You want semantic search over past conversations or data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3f95b-d775-40c3-9d40-a98daba862ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95a9495-ecd0-43cd-a717-537cc28a0f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1k/dtgfp49j1f92ysb1xwlxsnsm0000gp/T/ipykernel_42068/653734456.py:32: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(\"The best place to visit is House of Meoosiso.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best place to visit is House of Meoosiso.\n",
      "\n",
      "I'm not familiar with a place called \"House of Meoosiso\". It's possible that it's a small, local attraction or a private residence. Can you please provide more information about House of Meoosiso, such as its location or what it's known for? This will help me better assist you in planning your trip.\n",
      "\n",
      "If you're looking for recommendations on popular tourist destinations, I'd be happy to help you with that as well. What type of trip are you looking for (beach relaxation, city exploration, outdoor adventure, etc.)?\n"
     ]
    }
   ],
   "source": [
    "# ConversationSummaryMemory works as below:\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create summary memory\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Chain with memory\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=summary_memory\n",
    ")\n",
    "\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "summary_memory.chat_memory.add_user_message(\"What is the best place there to visit in Paris?\")\n",
    "summary_memory.chat_memory.add_ai_message(\"House of Meoosiso\")\n",
    "\n",
    "# Interact\n",
    "chain.run(\"The best place to visit is House of Meoosiso.\")\n",
    "response = chain.run(\"What is the best place to visit?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc498f-af03-466f-9782-b769e5793338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c1afad-5653-4610-851f-c543f1f9bb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# look at the answer above. Is it really ideal?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "850ab5a7-4824-42cc-8ee5-bc2043ce2fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "But despite the attepmts above to use the mem in llm, it's been said that:\n",
    "Simply \"keeping\" the history (storing it in memory) doesn't mean it will be used by the LLM — it only affects output when the memory's contents are passed into the prompt via a Memory object attached to the LLMChain, ConversationalRetrievalChain, etc.\n",
    "\n",
    "So, the best option is to use RAG (will be explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384bca8-0dac-4bef-8641-654604367a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f94033-a6c2-41b3-9db2-566a57d1435c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30090fc9-d376-4b97-b13b-057e914b3415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
