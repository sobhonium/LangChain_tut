{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47908d-ff17-4cdc-9608-7c9572af85ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# April 2025\n",
    "# parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f5e78-c4e9-46ac-baa2-a966679ac5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bcad2-8175-42e5-9391-61802a8e0899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1st method: using .env file.\n",
    "load_dotenv()\n",
    "# Access them using os.getenv or os.environ\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 2nd method: using hard code\n",
    "# api_key = \"<put the api key here>\"\n",
    "# if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#     os.environ[\"GROQ_API_KEY\"] = api_key #getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d34159-8e73-4450-87e7-127381f80ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c9ca1-8d75-4450-8c3b-a2843797a385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2494a-0534-42db-89f9-310698580d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca3ff8-8102-48aa-bcb5-a4652c4911bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke('Tell me about the lunar calander')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff2df9-eac1-423c-bcf3-31324727823b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f92009-52f8-4777-a865-a4faea9c754e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    llm\n",
    "    # | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551106c-1a8b-4d5c-94a9-7e25d2415f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke('Tell me about the lunar calander')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047512c-c07a-4a32-b17e-4f9452f755c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25742102-ba63-47a5-944e-4ffa386fda09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a83d32-fc26-4f82-b4ad-a046f80e46f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60a672-a1a7-4337-a0b8-b2935170a98c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Without StrOutputParser:\n",
    "output = llm.invoke(\"What is the capital of France?\")\n",
    "print(output)  # You have to know how to extract it\n",
    "\n",
    "# With StrOutputParser:\n",
    "chain = llm | StrOutputParser()\n",
    "print('\\n\\n',chain.invoke(\"What is the capital of France?\"))  # Just gives you '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720eacf-6d7b-4341-a85c-ad2b167a1990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194f69d-93bc-4cea-8ada-09f9d6ab92d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JsonOutputParser, however,  is more interesting to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69840e-3a31-4101-8bba-bd6442f5086c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf3d64-89fd-4969-9be3-eb093ca98d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "chain = llm | JsonOutputParser()\n",
    "\n",
    "prompt = \"Give me a JSON object with your the information about France: { \\\"country\\\": \\\"\\\", \\\"capital\\\": \\\"\\\", \\\"population\\\": \\\"\\\" }\"\n",
    "\n",
    "output = chain.invoke(prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d9458-1e6c-434f-8228-46366e2e927c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53f821-c60c-4253-980b-d4301a59929c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# but it seams like pydanticOutputParser is the one to go for."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bc6f714-ec4a-439c-a44a-6202858889ef",
   "metadata": {},
   "source": [
    "What is PydanticOutputParser?\n",
    "\n",
    "PydanticOutputParser is a LangChain parser that:\n",
    "\n",
    "Parses an LLM's JSON-like output\n",
    "Validates & casts it into a Pydantic model (Python dataclass with validation)\n",
    "Ensures the output follows a defined schema — super useful for reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d37f4-7347-4daf-93f3-c9ca6fa6d263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ce903-9106-466c-9bd3-cf3044db32f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Country(BaseModel):\n",
    "    name: str = Field(description=\"The official name of the country\")\n",
    "    capital: str = Field(description=\"The capital city\")\n",
    "    population: int = Field(description=\"Population count\")\n",
    "    currency: str = Field(description=\"Official currency\")\n",
    "    continent: str = Field(description=\"Continent where the country is located\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8a93e-d35d-4f1d-9d4c-5b64a59f0f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the parser\n",
    "parser = PydanticOutputParser(pydantic_object=Country)\n",
    "\n",
    "# Generate format instructions\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Provide information about the country '{country_name}' in the following format:\\n{format_instructions}\",\n",
    "    input_variables=[\"country_name\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c6a22-0a9d-49ab-9d77-de9c39b1709b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the chain by combining the prompt, LLM, and parser\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Invoke the chain with a specific country\n",
    "result = chain.invoke({\"country_name\": \"New Zealand\"})\n",
    "\n",
    "# Output the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf53126-38e4-4022-945c-49b1f46e98c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb090776-711d-4ec1-9c24-97415bb182f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fd1f7-cba6-427a-986a-08102c93b8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f6a19-ed91-4c58-b5c6-344738e45c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StructuredOutputParser"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c72d1a7-64c8-44ed-bc15-2bc9829de82a",
   "metadata": {},
   "source": [
    "StructuredOutputParser: Simplifying Structured Data Extraction\n",
    "\n",
    "The StructuredOutputParser is another tool in LangChain that simplifies the extraction of structured data from LLM outputs. It allows you to define a schema and ensures that the LLM's output adheres to this structure.​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55977f88-f762-4f92-a78d-8d6212e09efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957f536-24e7-440e-9ea5-0c6fccc99893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define response schemas\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"name\", description=\"Official name of the country\"),\n",
    "    ResponseSchema(name=\"capital\", description=\"Capital city\"),\n",
    "    ResponseSchema(name=\"population\", description=\"Total population\")\n",
    "]\n",
    "\n",
    "# Initialize the parser with response schemas\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Create a prompt template with format instructions\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Provide information about {country_name}.\\n{format_instructions}\",\n",
    "    input_variables=[\"country_name\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# # Initialize the language model\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Combine the prompt and model\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Invoke the chain with a specific country\n",
    "result = chain.invoke({\"country_name\": \"New Zealand\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f2c67-ee39-4979-8bfe-33b5dac1763d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc3e5d-c287-4b61-a6fa-0169f48ace51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7cfb1713-124d-4084-8977-208b7ee69ed6",
   "metadata": {},
   "source": [
    "if you look at https://nanonets.com/blog/langchain/?utm_source=chatgpt.com,  this piping is explained as a declarative way to easily compose modules together, where it says:\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# Example chain\n",
    "chain = ChatPromptTemplate() | ChatOpenAI() | CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909ac59-2d7f-4151-b5fd-b2de8071044e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9689fb-ac37-406b-87b5-1fba4102509c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7828d-ce0f-44f4-a2cf-fe9661b8f5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf2aae-ff73-43f7-9de3-bc52b05c6d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f7f86-16e1-4ab2-bd53-6da6d7c7c39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0d446-379e-468a-bcd3-69159bf28e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
