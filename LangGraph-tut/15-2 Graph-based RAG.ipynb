{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351c61b-cfdb-4069-8a78-70705b3b5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May 2025\n",
    "# transforming documents into graph-based representations using a large language model (LLM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d358e-7c8d-48bf-93c0-c99cd2d96f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1st method: using .env file.\n",
    "load_dotenv()\n",
    "# Access them using os.getenv or os.environ\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 2nd method: using hard code\n",
    "# api_key = \"<put the api key here>\"\n",
    "# if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#     os.environ[\"GROQ_API_KEY\"] = api_key #getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf295968-d9dd-4a62-b522-ae5a8373a607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab8570-63be-4563-b36a-0c5ceb8f145c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241b7c7-b6e4-4688-8f7f-98428381e7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825e38b-de80-4065-8413-a979653f1356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340408bc-f3f7-4037-ac47-8e72f97a65a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5e484-103b-4f87-9cbe-a23010e5c39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text_chunk\"],\n",
    "    template=\"\"\"\n",
    "Extract entities and relationships from the following text. \n",
    "Return them in a structured format like this:\n",
    "\n",
    "Entities:\n",
    "- Entity1\n",
    "- Entity2\n",
    "...\n",
    "\n",
    "Relationships:\n",
    "- (Entity1) --[relationship_type]--> (Entity2)\n",
    "...\n",
    "\n",
    "Text:\n",
    "{text_chunk}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0328f22-111a-4184-855f-f1850de8196e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305de10-ee75-417d-b9aa-a2e49f056f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f857a49-701e-435f-92f0-fa440d40d49e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db98dc-7450-4133-8a65-27f3516ba8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d85cf1-6057-42a7-b5a8-bb5622d3bb58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = TextLoader(\"my_doc2.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a21400-a873-4963-888a-97d90ef9ef95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3be37-6c98-44a3-bc88-6eb9e0af097a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for chunk in chunks:\n",
    "    result = chain.run(text_chunk=chunk.page_content)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c8070-761c-4747-847a-e6a5e0e47878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa407762-eef6-4da6-87d9-f979ca8a7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499ff11-7e76-4aa0-9159-1d231783d0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86061f8-6290-47cf-a87c-39313caf769b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0df5e099-77a3-450d-8a25-2618743b8846",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d58921-5230-4f78-8d9e-d15948363d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a88493-4d53-4eab-93c7-4a4410a29bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc244625-71e7-4e8d-9534-e18228737dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for res in results:\n",
    "    lines = res.splitlines()\n",
    "    for line in lines:\n",
    "        if \"--[\" in line:\n",
    "            parts = line.strip(\"()\").split(\"--[\")\n",
    "            entity1 = parts[0].strip()\n",
    "            relation, entity2 = parts[1].split(\"]-->\")\n",
    "            relation = relation.strip()\n",
    "            entity2 = entity2.strip(\"()\")\n",
    "            G.add_edge(entity1, entity2, label=relation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958b8f4-e727-49d4-81eb-d14b6b0901b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ea6e1-b8b6-4bb4-8b3f-e445f4ba5422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e7b6f-b6e6-42cc-a075-d14fbb27cd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c86e42-9d10-4404-91e0-ba07574636a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c5b3b82a-9bc9-4aa6-8c3e-47415cefaa45",
   "metadata": {},
   "source": [
    "Now, letâ€™s walk through how to integrate a graph-based document representation with RAG (Retrieval-Augmented Generation) in LangChain. This setup lets you combine the semantic depth of knowledge graphs with the generative power of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34eab8a-03c8-4845-ab05-796c63b29a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ae450-ec9b-46a5-9eb3-f08199dde857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rest is rubnish. The best path from here is to implement a Knowldege Graph RAG.\n",
    "# Still studying that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87759314-58e0-497d-ab36-681bac76b886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73489677-09a2-454a-b54c-cd9de47d1f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3381d4f-3740-41f1-b448-e44e89654df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40446baa-70e2-4c75-8f86-06438f59baa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc93a89-baa5-420b-a0d2-ce602006a0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Imports ---\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from typing import List\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "# # --- 2. Set API Key ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# --- 3. Load and Split Document ---\n",
    "loader = TextLoader(\"my_doc2.txt\")  # Load your text file\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# --- 4. Triple Extraction Prompt ---\n",
    "triple_prompt = PromptTemplate(\n",
    "    input_variables=[\"text_chunk\"],\n",
    "    template=\"\"\"\n",
    "Extract subject-predicate-object triples from the following text.\n",
    "Return in format: (Subject) --[Relation]--> (Object)\n",
    "\n",
    "Text:\n",
    "{text_chunk}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "extractor_chain = LLMChain(llm=llm, prompt=triple_prompt)\n",
    "\n",
    "# --- 5. Extract Triples ---\n",
    "triples = []\n",
    "for chunk in chunks:\n",
    "    output = extractor_chain.run(text_chunk=chunk.page_content)\n",
    "    triples.extend(output.strip().split(\"\\n\"))\n",
    "\n",
    "# --- 6. Build Knowledge Graph ---\n",
    "G = nx.DiGraph()\n",
    "for triple in triples:\n",
    "    if \"--[\" in triple and \"]-->\" in triple:\n",
    "        try:\n",
    "            subj = triple.split(\")\")[0].strip(\"(\").strip()\n",
    "            pred = triple.split(\"--[\")[1].split(\"]\")[0].strip()\n",
    "            obj = triple.split(\"]-->\")[1].strip(\"() \").strip()\n",
    "            G.add_edge(subj, obj, label=pred)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# --- 7. Graph-Based Retriever ---\n",
    "class GraphRetriever(BaseRetriever):\n",
    "    graph: nx.DiGraph\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        results = []\n",
    "        for node in self.graph.nodes:\n",
    "            if query.lower() in node.lower():\n",
    "                for neighbor in self.graph.neighbors(node):\n",
    "                    relation = self.graph.get_edge_data(node, neighbor)[\"label\"]\n",
    "                    sentence = f\"{node} {relation} {neighbor}.\"\n",
    "                    results.append(Document(page_content=sentence))\n",
    "        return results\n",
    "\n",
    "retriever = GraphRetriever(graph=G)\n",
    "\n",
    "# --- 8. Custom Prompt (to limit LLM hallucination) ---\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Use ONLY the information below to answer the question.\n",
    "If the answer is not contained, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# --- 9. RAG Pipeline ---\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": qa_prompt}\n",
    ")\n",
    "\n",
    "# --- 10. Run RAG QA ---\n",
    "question = \" billion freezing?\"\n",
    "answer = rag_chain.run(question)\n",
    "\n",
    "print(f\"\\nQ: {question}\")\n",
    "print(f\"A: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672a4ff-42fe-4874-95c5-b1ab4f67ffb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da9c35-f970-45db-84eb-b234a9ed9a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t in triples:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7588a-2478-41b5-880f-02921c7b178a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a66064-8255-4f43-a2c2-d98195654bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbdb79-80b6-4adc-8041-cb5910ec94b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc41f9-98a5-4094-a06f-3d50636b939d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Graph Retriever\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List\n",
    "import networkx as nx\n",
    "\n",
    "class GraphRetriever(BaseRetriever):\n",
    "    graph: nx.DiGraph  # ðŸ‘ˆ Explicitly declare it for Pydantic\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        relevant_docs = []\n",
    "        for node in self.graph.nodes:\n",
    "            if query.lower() in node.lower():\n",
    "                neighbors = self.graph.neighbors(node)\n",
    "                for neighbor in neighbors:\n",
    "                    relation = self.graph.get_edge_data(node, neighbor)['label']\n",
    "                    doc = Document(page_content=f\"({node}) --[{relation}]--> ({neighbor})\")\n",
    "                    relevant_docs.append(doc)\n",
    "        return relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d514d33-7d31-4319-94cb-55d1fb50b31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b244db-d6e4-4871-84c1-ec9c17481549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "retriever = GraphRetriever(graph=G)  # âœ… Now this will work\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa79d62-bc59-499b-b336-3e2a7469d54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571887e-4427-450e-801d-3f489a7d90c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What Trump did?\"\n",
    "response = rag_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27903b6-c1a6-402c-a571-6d9f0a9586ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b2c21-18d7-4186-8a79-f7819abf90c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96566feb-2f6e-41a1-b922-885ff4c1ee5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd4648-27db-4799-b4ba-bc9d03d0a5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "from typing import List\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# --- 1. Set your OpenAI API key ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"  # Replace with your actual key\n",
    "\n",
    "# --- 2. Load and split document ---\n",
    "loader = TextLoader(\"my_doc2.txt\")  # Replace with your file\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# --- 3. Triple extraction prompt ---\n",
    "triple_prompt = PromptTemplate(\n",
    "    input_variables=[\"text_chunk\"],\n",
    "    template=\"\"\"\n",
    "Extract clear subject-verb-object triples from the following text.\n",
    "\n",
    "Example:\n",
    "Text: \"Einstein developed the theory of relativity.\"\n",
    "Output: (Einstein) --[developed]--> (theory of relativity)\n",
    "\n",
    "Text:\n",
    "{text_chunk}\n",
    "\"\"\"\n",
    ")\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "extractor_chain = LLMChain(llm=llm, prompt=triple_prompt)\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# --- 4. Extract triples ---\n",
    "triples = []\n",
    "for chunk in chunks:\n",
    "    response = extractor_chain.run(text_chunk=chunk.page_content)\n",
    "    triples.extend(response.strip().splitlines())\n",
    "\n",
    "# --- 5. Build the graph ---\n",
    "G = nx.DiGraph()\n",
    "facts = []  # For FAISS vector store\n",
    "\n",
    "for triple in triples:\n",
    "    if '--[' in triple and ']-->' in triple:\n",
    "        try:\n",
    "            subj = triple.split(')')[0].strip('(').strip()\n",
    "            pred = triple.split('--[')[1].split(']')[0].strip()\n",
    "            obj = triple.split(']-->')[1].strip('() ').strip()\n",
    "            G.add_edge(subj, obj, label=pred)\n",
    "            fact = f\"{subj} {pred} {obj}.\"\n",
    "            facts.append(Document(page_content=fact))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# --- 6. Embed the graph facts using FAISS ---\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(facts, embedding_model)\n",
    "\n",
    "# --- 7. Semantic retriever using FAISS over graph facts ---\n",
    "class GraphSemanticRetriever(BaseRetriever):\n",
    "    vectorstore: FAISS\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "retriever = GraphSemanticRetriever(vectorstore=vectorstore)\n",
    "\n",
    "# --- 8. Constrained prompt to reduce hallucinations ---\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Answer the question using ONLY the context below.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# --- 9. Build the RAG QA chain ---\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": qa_prompt}\n",
    ")\n",
    "\n",
    "# --- 10. Ask a question ---\n",
    "question = \"What did Einstein develop?\"\n",
    "answer = rag_chain.run(question)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f600f6-5e79-4584-b233-bd337f2c8d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40014b36-acff-4566-ac68-abd5d794483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5dfc8-d1a1-4648-9a57-0bee2271ebb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f319e07-88c6-493c-9799-bf33468676f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfdad45-1c8a-4b6d-aca6-193597e08ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbaf79-e564-4e29-83ba-ea38cc2c52f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Imports ---\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from typing import List\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "\n",
    "# --- 3. Load and Split Document ---\n",
    "loader = TextLoader(\"my_doc.txt\")  # Replace with your text file path\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# --- 4. Triple Extraction Prompt ---\n",
    "triple_prompt = PromptTemplate(\n",
    "    input_variables=[\"text_chunk\"],\n",
    "    template=\"\"\"\n",
    "Extract subject-predicate-object triples from the following text.\n",
    "Return in format: (Subject) --[Relation]--> (Object)\n",
    "\n",
    "Text:\n",
    "{text_chunk}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "extractor_chain = LLMChain(llm=llm, prompt=triple_prompt)\n",
    "\n",
    "# --- 5. Extract Triples from Chunks ---\n",
    "all_triples = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    response = extractor_chain.run(text_chunk=chunk.page_content)\n",
    "    all_triples.append(response)\n",
    "\n",
    "# --- 6. Build Knowledge Graph ---\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for response in all_triples:\n",
    "    for line in response.splitlines():\n",
    "        if '--[' in line and ']-->' in line:\n",
    "            try:\n",
    "                subj = line.split(')')[0].strip('(').strip()\n",
    "                pred = line.split('--[')[1].split(']')[0].strip()\n",
    "                obj = line.split(']-->')[1].strip('() ').strip()\n",
    "                G.add_edge(subj, obj, label=pred)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# --- 7. Custom GraphRetriever with Rich Sentences ---\n",
    "class GraphRetriever(BaseRetriever):\n",
    "    graph: nx.DiGraph\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        relevant_docs = []\n",
    "        for node in self.graph.nodes:\n",
    "            if query.lower() in node.lower():\n",
    "                neighbors = self.graph.neighbors(node)\n",
    "                for neighbor in neighbors:\n",
    "                    relation = self.graph.get_edge_data(node, neighbor)['label']\n",
    "                    # Rich sentence instead of raw triple\n",
    "                    sentence = f\"{node} is related to {neighbor} via '{relation}'.\"\n",
    "                    relevant_docs.append(Document(page_content=sentence))\n",
    "        return relevant_docs\n",
    "\n",
    "retriever = GraphRetriever(graph=G)\n",
    "\n",
    "# --- 8. Constrained Prompt to Force Use of Retrieved Facts ---\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant. Use ONLY the information below to answer the question.\n",
    "If the answer is not present, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# --- 9. RAG Chain Setup ---\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": qa_prompt}\n",
    ")\n",
    "\n",
    "# --- 10. Ask Questions ---\n",
    "query = \"Why student visas are retracted?\"\n",
    "answer = rag_chain.run(query)\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5096f2-b8d4-47c6-9696-b227e65a5e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
