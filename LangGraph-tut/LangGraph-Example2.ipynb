{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54aa7394-5e42-423f-9f96-3c06c920a34a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# June 2025\n",
    "# AI-powered tweet improvement tool, using messageGraph in LangGraph\n",
    "# pretends that Humman is giving feedback to offer a far better and fine-tuned tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85aef7d-1241-4668-8297-53d14df8e539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541886e6-9608-4da7-8b1f-f27b9072caf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5454eadc-ac87-4512-ac0b-a25bf1739b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1st method: using .env file.\n",
    "load_dotenv()\n",
    "# Access them using os.getenv or os.environ\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 2nd method: using hard code\n",
    "# api_key = \"<put the api key here>\"\n",
    "# if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#     os.environ[\"GROQ_API_KEY\"] = api_key #getpass.getpass(\"Enter API key for Groq: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9e628e-416c-4fc7-b173-9cf1a5407650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc30187-c82a-4870-987b-9f59cca15d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8510c-7ada-4265-b375-9a9ee55c980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.graph import MessageGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import MessageGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def start(messages=None):\n",
    "    return HumanMessage(content=\"write a one-line tweet about a random topic.\")\n",
    "\n",
    "def generate(messages=None):\n",
    "    if len(messages)==1:        \n",
    "        all_msgs =messages[0].content #HumanMessage(content=\"write a one-line tweet about world war 2.\")\n",
    "        return AIMessage(content=llm.invoke([all_msgs]).content)\n",
    "\n",
    "#     # all_msgs = messages\n",
    "#     all_msgs = []\n",
    "#     for i, msg in enumerate(messages):\n",
    "        \n",
    "#         # if i<2:\n",
    "#         #     all_msgs.append(msg)\n",
    "#         #     continue\n",
    "\n",
    "            \n",
    "            \n",
    "#         if i%2==0:\n",
    "#             all_msgs.append(HumanMessage(content=f\"{msg.content}\"))\n",
    "#             continue\n",
    "        \n",
    "#         # if i%2==1:\"\n",
    "#         all_msgs.append(AIMessage(content=f\"{msg.content}\"))\n",
    "        \n",
    "#     print('-'*10)\n",
    "#     print('all_msgs=', all_msgs)\n",
    "\n",
    "#     response= llm.invoke(all_msgs)\n",
    "#     # print('-'*10)\n",
    "#     # print(all_msgs)\n",
    "#     print('='*10)\n",
    "    print('-'*10)\n",
    "    \n",
    "    # print(messages)\n",
    "    print('='*10)\n",
    "    \n",
    "#     for msg in messages:\n",
    "#         if isinstance(msg, HumanMessage):\n",
    "       \n",
    "#             print(\"(Hu) -->  \", msg.content)\n",
    "#         else:\n",
    "#             print('(AI) -->  ', msg.content)\n",
    "#         \n",
    "        \n",
    "\n",
    "    response = llm.invoke(f'improve this tweet: {messages[-1].content}, and only give one-line tweet without explaining/mentioning what you changing')\n",
    "    # if len(messages)%1==0:\n",
    "        \n",
    "    # return HumanMessage(content=f\"how about this: {response.content}\")\n",
    "    # print('response.content=', response.content)\n",
    "    return AIMessage(content=f\"a better tweet is : {response.content}\")\n",
    "\n",
    "def reflect(messages):\n",
    "    response = llm.invoke(f'improve this tweet: {messages[-1].content}, and only give one-line tweet without explaining/mentioning what you changing')\n",
    "    return HumanMessage(content=f\"{response.content}. Can you improve it more?\")\n",
    "\n",
    "def should_continue(msg):\n",
    "    # print('msg[generate node]=',msg[\"generate_node\"])\n",
    "    print('len(msg)=', len(msg))\n",
    "    if (len(msg)>30):\n",
    "        return \"to_final\"\n",
    "    return \"to_continue\"\n",
    "\n",
    "def final(messages):\n",
    "        print('='*10)\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "\n",
    "                print(\"(Hu) -->  \", msg.content)\n",
    "            else:\n",
    "                print('(AI) -->  ', msg.content)\n",
    "        print('='*10)\n",
    "        response = llm.invoke(messages[:-1]) # technically the last must be a hummanMessage\n",
    "        print('\\n\\nbased on all the corrections the final tweet would be:', response.content)\n",
    "    \n",
    "graph = MessageGraph()\n",
    "# graph.add_edge(START, \"generate\")\n",
    "graph.add_node(\"start\", start)\n",
    "graph.add_node(\"generate\", generate)\n",
    "graph.add_node(\"reflect\", reflect)\n",
    "graph.add_node(\"final\", final)\n",
    "graph.add_edge(\"start\",\"generate\")\n",
    "graph.set_entry_point(\"start\")\n",
    "\n",
    "graph.add_conditional_edges(\"generate\", \n",
    "                            should_continue,\n",
    "                            \n",
    "                          {    \n",
    "        \"to_continue\": \"reflect\",  # Mapping function output to nodes\n",
    "        \"to_final\": \"final\",\n",
    "    },     \n",
    "                           )\n",
    "graph.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# # Run the graph with a dummy input\n",
    "# graph.invoke([HumanMessage(content=\"Go!\")])\n",
    "\n",
    "output = app.invoke([], config={\"recursion_limit\": 50})\n",
    "\n",
    "for msg in output:\n",
    "    msg.pretty_print()\n",
    "    \n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98005406-8bcf-4501-8e60-d20164c5a8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64865dc-c3e4-40a4-a540-f3c16123beef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
