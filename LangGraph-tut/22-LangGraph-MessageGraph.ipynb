{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd763d-e4df-4249-afa3-940ff6c188c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91bc42-812a-42db-87a6-9823f9d91e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fab5d2-bbf3-4128-b2e7-5c77a9404872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa1978-2136-4446-9644-4ea3fefe68ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not sure about the following. I guesss it's wrong.\n",
    "# You can access messageGraph through message[\"<node_name>\"]\n",
    "# in the following for example one node is called 'start'\n",
    "# you can access it through messages[\"start\"] next time \n",
    "# you need to access that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a2a9d-48d1-49c3-ab07-f669a639154c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "68d7b992-3a1b-47e5-a666-4fb1a44e2673",
   "metadata": {},
   "source": [
    " What is a MessageGraph?\n",
    "\n",
    "MessageGraph is a flow-based graph where the nodes pass around chat messages (like HumanMessage, AIMessage, etc.) instead of regular states or variables. It's designed for workflows that look like conversations or chains of chat interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed360e7-db22-4dea-ae7d-039e40256f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b39b7-7812-47d4-8339-57b4565a1daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb8c60a-ec52-44a3-999a-e3b5e091bbff",
   "metadata": {
    "tags": []
   },
   "source": [
    "The main difference between MessageGraph and StateGraph:\n",
    "\n",
    "- `result = messages[\"double\"][0].content` for accessing StateGraph\n",
    "- `result = messages[0].content` for accessing MessageGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc2bb8-219d-42a3-bfe0-eb998fa4f6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0902c-daa5-4bb6-a9da-c3d9abb68e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "47455d76-01f6-4ce1-aaa4-d3bc21c9179a",
   "metadata": {},
   "source": [
    "the messages saved in MessageGraph has a style like below:\n",
    "    \n",
    "    messages = [\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What's the population?\")\n",
    "]\n",
    "\n",
    "    and to access each of them, you need to access them through index calling like messages[2]\n",
    "    another fact is that HumanMessage and HumanMessage should be in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a742fae-942e-44e9-ae24-76593ef260ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f4d54fea-d262-4094-85d4-e9e9a78f02e7",
   "metadata": {},
   "source": [
    "what a function like below does in MessageGraph is quite easy. It recieve the whole messages(another name optionally) so far and return the next one to it.\n",
    "In the following for example the whole conversation (messages so far in the graph) are given\n",
    "to this function and when it's done AIMessage(content=\"doubllllly\") is added to it.\n",
    "For example if so far before running this function in grpah the messages are [HummanMessage(content=\"hi stupid\")] after running this, the whole convesation will be \n",
    "\n",
    "[\n",
    "HummanMessage(content=\"hi stupid\"),\n",
    "AIMessage(content=\"doubllllly\")\n",
    "]\n",
    "\n",
    "def double(messages):\n",
    "    print('double node entered')\n",
    "    # Access the message \n",
    "    msg = messages[0].content \n",
    "    doubled = msg \n",
    "    return AIMessage(content=\"doubllllly\")  # Return \"10\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d23ea-5585-44f2-b726-b3cb54b2cfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f38ef95a-d24b-4811-92e6-17caf331e625",
   "metadata": {},
   "source": [
    "in LangGraph a list of message of like this:\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What's the population?\")\n",
    "]\n",
    "\n",
    "is named MessageState (don't confuse it for MessageGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e2c07-e7c5-4e4f-aaee-d424da34b5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b178eeb-3272-412b-a925-f892221110e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df897f-92a8-4cfe-b1fa-762dcefca526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at how HumanMessage are used even if we return and work with strings...\n",
    "# By defualt returning strings means HumanMessage(content=\"value\")\n",
    "\n",
    "# return \"value\" ==== HumanMessage(content=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74651b4c-e1b0-404b-9131-225ec138105b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Node 1: Send a number\n",
    "def start(_input=None):\n",
    "    print('start node entered')\n",
    "    return \"starrrrrrt\"#HumanMessage(content=\"5\")\n",
    "\n",
    "# Node 2: Double the number\n",
    "def double(messages):\n",
    "    print('double node entered')\n",
    "    # Access the message \n",
    "    msg = messages[0].content #messages[\"start\"][0].content  # This will be \"5\"\n",
    "    doubled = msg #int(msg) * 2\n",
    "    # return AIMessage(content=str(doubled))  # Return \"10\"\n",
    "    return \"doubllllly\"  # Return \"10\"\n",
    "\n",
    "# Node 3: Print the result\n",
    "def final(messages):\n",
    "    print('final node entered')\n",
    "    result = messages[0].content #messages[\"double\"][0].content  # This will be \"10\"\n",
    "    # print(\"Final result is:\", result)\n",
    "    # print('all messages:', '-'*10)\n",
    "    # print(messages)\n",
    "    # print('Length of messages:', len(messages))\n",
    "    # print('all messages:', '-'*10)\n",
    "    return \"Finallllly\"\n",
    "    \n",
    "\n",
    "\n",
    "# Build the message graph\n",
    "graph = MessageGraph()\n",
    "graph.add_node(\"start\", start)\n",
    "graph.add_node(\"double\", double)\n",
    "graph.add_node(\"final\", final)\n",
    "\n",
    "# Define edges\n",
    "graph.set_entry_point(\"start\")\n",
    "graph.add_edge(\"start\", \"double\")\n",
    "graph.add_edge(\"double\", \"final\")\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# # Run the graph with a dummy input\n",
    "# graph.invoke([HumanMessage(content=\"Go!\")])\n",
    "\n",
    "app.invoke([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf24fd-67d6-48c6-adc6-161a12a2229d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b911b-73c3-4d51-9d92-5a9fc6cc27ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    \n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1520c-9898-4357-b8c5-699e6ca7c1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f11b19-2850-4a85-8cea-3762a45a79d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In here I clearly use\n",
    "# return HumanMessage(content=\"5\")\n",
    "# than \n",
    "# return \"5\"\n",
    "# and laso use AIMessage() when I needed to than only returning strings.\n",
    "\n",
    "# the same for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb36b88-ab68-4ece-9453-a6ecbaec0084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Node 1: Send a number\n",
    "def start(_input=None):\n",
    "    return HumanMessage(content=\"5\")\n",
    "\n",
    "# Node 2: Double the number\n",
    "def double(messages):\n",
    "    # Access the message \n",
    "    msg = messages[0].content #messages[\"start\"][0].content  # This will be \"5\"\n",
    "    doubled = msg #int(msg) * 2\n",
    "    # return AIMessage(content=str(doubled))  # Return \"10\"\n",
    "    return AIMessage(content=doubled)  # Return \"10\"\n",
    "\n",
    "# Node 3: Print the result\n",
    "def final(messages):\n",
    "    result = messages[0].content #messages[\"double\"][0].content  # This will be \"10\"\n",
    "    print(\"Final result is:\", result)\n",
    "    print('all messages:', '-'*10)\n",
    "    print(messages)\n",
    "    print('Length of messages:', len(messages))\n",
    "    print('all messages:', '-'*10)\n",
    "    return AIMessage(content=\"Done\")\n",
    "    \n",
    "\n",
    "\n",
    "# Build the message graph\n",
    "graph = MessageGraph()\n",
    "graph.add_node(\"start\", start)\n",
    "graph.add_node(\"double\", double)\n",
    "graph.add_node(\"final\", final)\n",
    "\n",
    "# Define edges\n",
    "graph.set_entry_point(\"start\")\n",
    "graph.add_edge(\"start\", \"double\")\n",
    "graph.add_edge(\"double\", \"final\")\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# # Run the graph with a dummy input\n",
    "# graph.invoke([HumanMessage(content=\"Go!\")])\n",
    "\n",
    "app.invoke([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59147b58-11d5-4b88-ab56-6376ced06939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c040ee0-3041-4c1c-9d7e-8fb0ca062ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a1b9c-aff5-44d5-bdd0-209adfa1a94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def one(_input=None):\n",
    "    return HumanMessage(content=\"1\")\n",
    "\n",
    "def two(messages):\n",
    "    return AIMessage(content=\"two\")\n",
    "\n",
    "def three(messages):\n",
    "    return AIMessage(content=\"three\")\n",
    "\n",
    "\n",
    "graph = MessageGraph()\n",
    "graph.add_node(\"one\", one)\n",
    "graph.add_node(\"two\", two)\n",
    "graph.add_node(\"three\", three)\n",
    "\n",
    "graph.set_entry_point(\"one\")\n",
    "graph.add_edge(\"one\", \"two\")\n",
    "graph.add_edge(\"two\", \"three\")\n",
    "graph.add_edge(\"three\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# # Run the graph with a dummy input\n",
    "# graph.invoke([HumanMessage(content=\"Go!\")])\n",
    "\n",
    "app.invoke([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0b35a-a166-43d9-8863-2ee6658fdb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6efe1-b050-4f5f-94f7-acea7415c7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    \n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcc566-80e9-4ff6-82ea-2b9e9391ce41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856881c-0a49-48c3-82b7-43f3bc1f2377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ae5a7b-a8e3-4659-acfc-ac5a33d0edff",
   "metadata": {
    "tags": []
   },
   "source": [
    "now kowing that an llm works like this:\n",
    "\n",
    "```python\n",
    "llm = ChatGroq(temperature=0, model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Define a conversation history\n",
    "messages = [\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What's the population?\")\n",
    "]\n",
    "\n",
    "# Send the conversation to the LLM\n",
    "response = llm.invoke(messages)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8821d-747a-4678-a3f7-9fc750f0ab60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd2a4a-bde9-44e0-8204-fabad81ffd5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a conversation history\n",
    "# but you should know something in here. when using a messages like bellow, \n",
    "# the order should be taken seriously, otherwise the llm return ''\n",
    "# for the two cases comment and use 'messages'\n",
    "# The order should be:[HumanMessage, AIMessage, HumanMessage, ...]\n",
    "# somethinglike [HumanMessage, AIMessage, AIMessage, ....] is not accepted.\n",
    "    \n",
    "messages = [\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What's the population?\")\n",
    "]\n",
    "\n",
    "messages = [\n",
    "HumanMessage(content='Hi. how is going?', additional_kwargs={}, response_metadata={}, id='f1daec05-963e-41f4-a899-f140e5e76d2a'), \n",
    "AIMessage(content=\"I'm just an AI model and there is no mood defined for me.\", additional_kwargs={}, response_metadata={}, id='efa2603d-6cae-467c-885b-92331035a165'), \n",
    "AIMessage(content='OK. you are stupid.', additional_kwargs={}, response_metadata={}, id='16bdafe1-fc77-4ca4-95f9-e59a26ec1dee')\n",
    "]\n",
    "\n",
    "# Send the conversation to the LLM\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4bd4b-bda5-4acb-831c-7e3dfa562262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5964767-967a-4e02-9dba-9f78d8fed7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd73e1-b9b1-4196-82da-0c9813a012f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1st method: using .env file.\n",
    "load_dotenv()\n",
    "# Access them using os.getenv or os.environ\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 2nd method: using hard code\n",
    "# api_key = \"<put the api key here>\"\n",
    "# if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#     os.environ[\"GROQ_API_KEY\"] = api_key #getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.graph import MessageGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def one(_input=None):\n",
    "    return HumanMessage(content=\"Hi. how is going?\")\n",
    "\n",
    "def two(messages):\n",
    "    return AIMessage(content=\"I'm just an AI model and there is no mood defined for me.\")\n",
    "\n",
    "def three(messages):\n",
    "    return AIMessage(content=\"OK. you are stupid.\")\n",
    "\n",
    "def final(messages):\n",
    "    # print('-'*10)\n",
    "    # print(messages)\n",
    "    '''\n",
    "    just to make something like bellow to send to llm.invoke(messages)\n",
    "    \n",
    "        messages = [\n",
    "        HumanMessage(content=\"What is the capital of France?\"),\n",
    "        AIMessage(content=\"The capital of France is Paris.\"),\n",
    "        HumanMessage(content=\"What's the population?\")\n",
    "    ]\n",
    "    \n",
    "    That's actually a standard way of doing so.\n",
    "    The order should be:[HumanMessage, AIMessage, HumanMessage, ...]\n",
    "    somethinglike [HumanMessage, AIMessage, AIMessage, ....] is not accepted.\n",
    "    \n",
    "    '''\n",
    "    # all_msgs = messages\n",
    "    all_msgs = []\n",
    "    for i, msg in enumerate(messages):\n",
    "        if i%2==0:\n",
    "            all_msgs.append(f'HummanMessage({msg.content})')\n",
    "            continue\n",
    "        \n",
    "        # if i%2==1:\n",
    "        all_msgs.append(f'AIMessage({msg.content})')\n",
    "        \n",
    "    # print('-'*10)\n",
    "    print('all_msgs=', all_msgs)\n",
    "\n",
    "    response= llm.invoke(all_msgs)\n",
    "    print('response.content=', response.content)\n",
    "    return AIMessage(content=response.content)\n",
    "\n",
    "graph = MessageGraph()\n",
    "graph.add_node(\"one\", one)\n",
    "graph.add_node(\"two\", two)\n",
    "graph.add_node(\"three\", three)\n",
    "graph.add_node(\"final\", final)\n",
    "\n",
    "graph.set_entry_point(\"one\")\n",
    "graph.add_edge(\"one\", \"two\")\n",
    "graph.add_edge(\"two\", \"three\")\n",
    "graph.add_edge(\"three\", \"final\")\n",
    "graph.add_edge(\"final\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# # Run the graph with a dummy input\n",
    "# graph.invoke([HumanMessage(content=\"Go!\")])\n",
    "\n",
    "app.invoke([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6049904-60d6-424e-81df-d4aa888ba2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72416271-a106-41ef-9920-955dc4e1348a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    " HumanMessage(content='write a one-line tweet about world war 2.', additional_kwargs={}, response_metadata={}, id='c5cff51d-eb66-4ee9-bfa5-54b5bbcc39f4'),\n",
    " AIMessage(content='\"70 years ago, the world came together to defeat the evil forces of Nazi Germany, Imperial Japan, and Fascist Italy in World War II, marking a pivotal moment in human history #WWII #Remembrance\"', additional_kwargs={}, response_metadata={}, id='79d3af4e-980a-4ddc-8798-fd5a026837bd'),\n",
    " HumanMessage(content='\"70 years ago, the world united against the tyranny of Nazi Germany, Imperial Japan, and Fascist Italy, shaping the course of human history forever #WWII #Remembrance\"', additional_kwargs={}, response_metadata={}, id='7ba7b428-666c-407c-98f4-4d5102fa39fe'),\n",
    " AIMessage(content='a better tweet is : \"On this day, 70 years ago, the world came together to defeat tyranny, forever changing the course of human history #WWII #Remembrance\"', additional_kwargs={}, response_metadata={}, id='23f20458-36e0-4d59-992b-5f8b644ffdf5'),\n",
    " HumanMessage(content='\"On this day, 70 years ago, nations united against tyranny, shaping humanity\\'s destiny forever #WWII #Remembrance\"', additional_kwargs={}, response_metadata={}, id='0d72175e-bc31-4310-9d32-e707fb5fcd95'),\n",
    " AIMessage(content='a better tweet is : \"On this D-Day anniversary, 70 years ago, nations rose against tyranny, forever reshaping humanity\\'s path #WWII #Remembrance\"', additional_kwargs={}, response_metadata={}, id='cf88bfa8-951a-4349-955a-4725d2bbe547')\n",
    "]\n",
    "           \n",
    "           \n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df685afa-27bb-4a52-a4b7-0b14034d1acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c0dd9-8340-4ed1-8f02-fae80078132c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm.invoke(\"\"\"when I print(HumanMessage(content=\"a\")),\n",
    "# it only gives: content='a' additional_kwargs={} response_metadata={}\n",
    "\n",
    "# but I want exactly: HumanMessage(content=\"a\").\n",
    "# tell me the answer by using isinstance \n",
    "\n",
    "# \"\"\").content.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f5670-66db-412d-8058-9df38f5f436f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93416ef1-e591-46b9-99e2-41a935e29a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878f117-7610-431e-bebe-1eeb2a529397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HumanMessage(content=\"write a one-line tweet about world war 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e985090-b10f-4bbd-bf34-8d87e492acd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(HumanMessage(content=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831af42e-fff6-427a-a3e6-e1beee2c6c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a972ec7-ccdb-46f0-90b7-6addf209a95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "        all_msgs = HumanMessage(content=\"write a one-line tweet about world war 2.\")\n",
    "        AIMessage(content=llm.invoke([all_msgs]).content)\n",
    "        all_msgs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca5ada-5abe-4a10-9ab6-d702338c71d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621f86f-4e0d-4ffa-8579-da4068a6d24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b76e67-8cb5-4eb6-a041-c3ee8e10b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretend that human is in the middle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2afa99-c42b-4ae0-af9c-ac06a21535d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1st method: using .env file.\n",
    "load_dotenv()\n",
    "# Access them using os.getenv or os.environ\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 2nd method: using hard code\n",
    "# api_key = \"<put the api key here>\"\n",
    "# if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#     os.environ[\"GROQ_API_KEY\"] = api_key #getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.graph import MessageGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import MessageGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def start(messages=None):\n",
    "    return HumanMessage(content=\"write a one-line tweet about a random topic.\")\n",
    "\n",
    "def generate(messages=None):\n",
    "    if len(messages)==1:        \n",
    "        all_msgs =messages[0].content #HumanMessage(content=\"write a one-line tweet about world war 2.\")\n",
    "        return AIMessage(content=llm.invoke([all_msgs]).content)\n",
    "\n",
    "#     # all_msgs = messages\n",
    "#     all_msgs = []\n",
    "#     for i, msg in enumerate(messages):\n",
    "        \n",
    "#         # if i<2:\n",
    "#         #     all_msgs.append(msg)\n",
    "#         #     continue\n",
    "\n",
    "            \n",
    "            \n",
    "#         if i%2==0:\n",
    "#             all_msgs.append(HumanMessage(content=f\"{msg.content}\"))\n",
    "#             continue\n",
    "        \n",
    "#         # if i%2==1:\"\n",
    "#         all_msgs.append(AIMessage(content=f\"{msg.content}\"))\n",
    "        \n",
    "#     print('-'*10)\n",
    "#     print('all_msgs=', all_msgs)\n",
    "\n",
    "#     response= llm.invoke(all_msgs)\n",
    "#     # print('-'*10)\n",
    "#     # print(all_msgs)\n",
    "#     print('='*10)\n",
    "    print('-'*10)\n",
    "    \n",
    "    # print(messages)\n",
    "    print('='*10)\n",
    "    \n",
    "#     for msg in messages:\n",
    "#         if isinstance(msg, HumanMessage):\n",
    "       \n",
    "#             print(\"(Hu) -->  \", msg.content)\n",
    "#         else:\n",
    "#             print('(AI) -->  ', msg.content)\n",
    "#         \n",
    "        \n",
    "\n",
    "    response = llm.invoke(f'improve this tweet: {messages[-1].content}, and only give one-line tweet without explaining/mentioning what you changing')\n",
    "    # if len(messages)%1==0:\n",
    "        \n",
    "    # return HumanMessage(content=f\"how about this: {response.content}\")\n",
    "    # print('response.content=', response.content)\n",
    "    return AIMessage(content=f\"a better tweet is : {response.content}\")\n",
    "\n",
    "def reflect(messages):\n",
    "    response = llm.invoke(f'improve this tweet: {messages[-1].content}, and only give one-line tweet without explaining/mentioning what you changing')\n",
    "    return HumanMessage(content=f\"{response.content}. Can you improve it more?\")\n",
    "\n",
    "def should_continue(msg):\n",
    "    # print('msg[generate node]=',msg[\"generate_node\"])\n",
    "    print('len(msg)=', len(msg))\n",
    "    if (len(msg)>6):\n",
    "        return \"to_final\"\n",
    "    return \"to_continue\"\n",
    "\n",
    "def final(messages):\n",
    "        print('='*10)\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "\n",
    "                print(\"(Hu) -->  \", msg.content)\n",
    "            else:\n",
    "                print('(AI) -->  ', msg.content)\n",
    "        print('='*10)\n",
    "        response = llm.invoke(messages[:-1]) # technically the last must be a hummanMessage\n",
    "        print('\\n\\nbased on all the corrections the final tweet would be:', response.content)\n",
    "    \n",
    "graph = MessageGraph()\n",
    "# graph.add_edge(START, \"generate\")\n",
    "graph.add_node(\"start\", start)\n",
    "graph.add_node(\"generate\", generate)\n",
    "graph.add_node(\"reflect\", reflect)\n",
    "graph.add_node(\"final\", final)\n",
    "graph.add_edge(\"start\",\"generate\")\n",
    "graph.set_entry_point(\"start\")\n",
    "\n",
    "graph.add_conditional_edges(\"generate\", \n",
    "                            should_continue,\n",
    "                            \n",
    "                          {    \n",
    "        \"to_continue\": \"reflect\",  # Mapping function output to nodes\n",
    "        \"to_final\": \"final\",\n",
    "    },     \n",
    "                           )\n",
    "graph.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# # Run the graph with a dummy input\n",
    "# graph.invoke([HumanMessage(content=\"Go!\")])\n",
    "\n",
    "output = app.invoke([])\n",
    "for msg in output:\n",
    "    msg.pretty_print()\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cbb310-44ce-4cc8-9e01-7687af7dfeff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61f90f-97bb-4ff2-b2a4-bef858a342aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945da4d5-784a-4a68-a13a-4dfe0c24c216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916b3d0-49cb-446b-847b-f2fa750f8d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c512a-c088-4ac7-8b88-a55381974d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    \n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08585a73-c174-4a4b-bced-fe20ef4babff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3550c2d-9b68-47f9-9460-5deecd27eb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292da3a-8954-4e7b-b2e1-9de83b767ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d394c6-f037-4467-b83b-6c5b58a21f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a10eb-0ebe-4c6c-aa5f-bb70784f706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f215c-ef83-4271-8238-24e4c28be5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b319854-0653-44e1-a4da-b23877fd7d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc10f61-559e-47a0-8fc0-4dd8741135ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a9ebf-614d-43f3-8d3c-181ecf294030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cbf64d-23cf-446f-b3b9-9661e6f76e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e1aaf-04cd-4214-b081-83b8ab0f2cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920607a-0301-4cea-8540-62bb7bc72fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560d6cd-53f7-430c-9b7d-1fe0152b0e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
